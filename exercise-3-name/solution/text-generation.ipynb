{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 50,085\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of characters: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we'll try for text generation will be by training the model and\n",
    "generating by character.\n",
    "\n",
    "This will mean each token will be a single character from the text and the model\n",
    "will learn to predict the next character (a token).\n",
    "\n",
    "To generate text the text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "genearate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "> - Skip?\n",
    "> - lowercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Can be multiple actions to normalize text\n",
    "    normalized_text = text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# normalized_text = normalize_text(raw_text)\n",
    "# print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # Effectively the same as a string\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# pretokenized_text = pretokenize_text(normalized_text)\n",
    "# print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "> Skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Input string and generated string\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.2764747702656463\n",
      "[00m 6.3s (0 0%) 1.9193]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be thee inthe to all haim cato recfoneb, eve ie to thee miands, and hit mren,\n",
      "yey brated nites lermes;\n",
      "Epoch 2/5, Loss: 1.9104908132705445\n",
      "[00m 12.8s (1 20%) 1.9763]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belavg winc.\n",
      "iplefess weser.\n",
      "\n",
      "ayled, ofe sort, am plyder and gardis't, i stecmong; arope sert, i are\n",
      "a\n",
      "Epoch 3/5, Loss: 1.8275297895407143\n",
      "[00m 19.1s (2 40%) 1.6987]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bein the prup to mar us the sever, go sto have heir why his are'd,\n",
      "and i's: be agiolids than spere sai\n",
      "Epoch 4/5, Loss: 1.7852085169892722\n",
      "[00m 25.6s (3 60%) 1.9129]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beard be were\n",
      "i shalld?\n",
      "\n",
      "blius canto love: vence anur!\n",
      "as for any inges o, he fle, where read eaks?\n",
      "\n",
      "a\n",
      "Epoch 5/5, Loss: 1.7580973786667893\n",
      "[00m 32.0s (4 80%) 2.1375]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to becoundinius stome nos.\n",
      "\n",
      "coziniunidius, wouk\n",
      "not offer\n",
      "are your cmalowrer heor bey you of sworkly hear\n"
     ]
    }
   ],
   "source": [
    "PHRASE = 'To be or not to be'\n",
    "epochs = 5\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to beether he eldielw.\n",
      "\n",
      "cominius:\n",
      "vird,\n",
      "unou welf to gol the selved,\n",
      "and not that denia in all fuch the b\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-Based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize (Choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "bert_model_name = 'bert-base-cased'\n",
    "bert_model_name_uncased = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    bert_model_name_uncased,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 16  # Tokens \n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_token(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # Input string and generated string\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(tokenized_text + generated_tokens)\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    input_str: str,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    ") -> str:\n",
    "    # Set model into \"evaluation mode\" (deactivates things like Dropout layers)\n",
    "    model.eval()\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    input_tensor = tokens_to_id_tensor(\n",
    "        tokens=tokenized_text,\n",
    "        token_id_mapping=token_mapping.token2id,\n",
    "    )\n",
    "\n",
    "    generated_text = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_tokens):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_token_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text.append(token_mapping.id2token(next_token_idx))\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_token_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "    # Convert to text again\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(generated_text)\n",
    "    output_str = input_str + ' ' + tokenizer.decode(output_ids)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.178692927905823\n",
      "[00m 2.6s (0 0%) 6.0003]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be mat :? done they him he com fl butcher, with : your your, the,nia : but it cannotrti he an will s - forth\n",
      "Epoch 2/5, Loss: 5.408855026365777\n",
      "[00m 5.3s (1 20%) 5.3355]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be am all upon leaves sword only our op may aside world is with you. all he lip ofs ve as, prayer. that us, is,\n",
      "Epoch 3/5, Loss: 4.877313198544393\n",
      "[00m 8.0s (2 40%) 4.4577]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be intend his yarn shericertius : i off, vol your most hostages way the behold rather em blood, vol interior : would what ta pity co\n",
      "Epoch 4/5, Loss: 4.477121763855871\n",
      "[00m 10.7s (3 60%) 4.6280]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be let to cominius, i shall back home we shall day with harm price ; where slip my incorporate for the souths than hat affection to your\n",
      "Epoch 5/5, Loss: 4.169655736635491\n",
      "[00m 13.4s (4 80%) 4.6953]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be them, and i - manner! corioli, bonnet to him are ; and dam enough. virgilia : i': ears you true,\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_token(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be the volsces are ; and your choice, the vols, and i am, and'd, and's ay? marcius : if\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_token(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "        topk=10,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-nlp-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
